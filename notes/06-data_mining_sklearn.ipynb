{
 "metadata": {
  "name": "",
  "signature": "sha256:6c703685b207db87021a24836e6ebb244c72ee35d4735777e9a2c4a513943b57"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Objectives\n",
      "- Understand the basics in a data pipeline model\n",
      "- Build helper functions to help standardize our processing with sklearn\n",
      "- Review the linear regression algorithm in detail\n",
      "- Explain challenges with linear regressions: multicollinearity, regularization techniques"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## How Sklearn works\n",
      "Scikit Learn (sklearn), a machine learning library for python, will do much of our heavy lifting. While sklearn has everything we've used in statsmodels, there are two distinct differences:\n",
      "\n",
      "1. sklearn is modular. We call sklearn modular because many of the classes (objects) we use can be easily switched out and replaced with other sklearn classes.\n",
      "2. sklearn's implementation feels more \"engineery.\" Instead of printing pretty tables, sklearn objects and their attributes return basic python and numpy data structures (floats/integers, lists, ndarrays, dictionaries)\n",
      "\n",
      "Therefore, while statsmodels is very functional as an exploratory tool (and for time series analysis!), sklearn can control our entire data mining workflow."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## What is the Data Mining pipeline?\n",
      "So far in the coursework we've uncovered the heavylifting and processing of data, be it pulling datasets together, cleaning it up, aggregating and visualising. While we can use tools like regressions to fill in our exploratory view, our objectives are also often to predict and define. In order to do so, we need to:\n",
      "\n",
      "- Understand the impact of our model. We define our model (for now) as the set of feature data (X) given a value or target to be predicted (y). In particular, we want a model that:\n",
      "    1. Is logical (we should not see, for example, an inverse relationship if the value in someone's home and their income).\n",
      "    2. Is statistically significant (low probabilities of significance due to randomness or chance)\n",
      "    3. Has strong qualifiers or predictors in the outcome (if everything is significant, but the objective values for each feature are small, then our model does not best explain the predicted value).\n",
      "- Evaluate the performance of our model by testing the predicted results\n",
      "- Determine if there are additional needs to build a stronger model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, we need a solution to:\n",
      "\n",
      "- Help us understand and automate the feature selection process. We can use the feature selection part of sklearn to do this.\n",
      "- Split our data into a training set to \"learn\" from, and a test set to \"evaluate.\" We can use the cross validation module of sklearn for this part.\n",
      "- Use an algorithm to learn and evaluate. For today, we'll stick with generalized linear models.\n",
      "- Optimize to some performance metric. With linear models, we've been using r-squared (R2), which is found in the metrics module.\n",
      "- Each class we use sklearn, we'll explore something new with feature selection or extraction, a cross validation technique, an algorithm, and review performance metrics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from __future__ import division\n",
      "pd.set_option('display.max_rows', 500)\n",
      "pd.set_option('display.max_columns', 50)\n",
      "pd.set_option('display.width', 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## import sklearn modules\n",
      "from sklearn import feature_selection as f_select\n",
      "from sklearn import linear_model as lm\n",
      "from sklearn import cross_validation as cv\n",
      "from sklearn import metrics\n",
      "from sklearn.datasets import load_boston\n",
      "\n",
      "boston = load_boston() ## load the sample dataset into a \"bunch\" object\n",
      "desc = boston.DESCR ## get the description text from the DESCR attribute\n",
      "print desc\n",
      "bostondf = pd.DataFrame(boston.data, columns=boston.feature_names) ## create a dataframe from the dataset object\n",
      "bostondf['MEDV'] = boston.target ## create a new column usign the target variable\n",
      "bostondf.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Boston House Prices dataset\n",
        "\n",
        "Notes\n",
        "------\n",
        "Data Set Characteristics:  \n",
        "\n",
        "    :Number of Instances: 506 \n",
        "\n",
        "    :Number of Attributes: 13 numeric/categorical predictive\n",
        "    \n",
        "    :Median Value (attribute 14) is usually the target\n",
        "\n",
        "    :Attribute Information (in order):\n",
        "        - CRIM     per capita crime rate by town\n",
        "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "        - INDUS    proportion of non-retail business acres per town\n",
        "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "        - NOX      nitric oxides concentration (parts per 10 million)\n",
        "        - RM       average number of rooms per dwelling\n",
        "        - AGE      proportion of owner-occupied units built prior to 1940\n",
        "        - DIS      weighted distances to five Boston employment centres\n",
        "        - RAD      index of accessibility to radial highways\n",
        "        - TAX      full-value property-tax rate per $10,000\n",
        "        - PTRATIO  pupil-teacher ratio by town\n",
        "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "        - LSTAT    % lower status of the population\n",
        "        - MEDV     Median value of owner-occupied homes in $1000's\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
        "\n",
        "This is a copy of UCI ML housing dataset.\n",
        "http://archive.ics.uci.edu/ml/datasets/Housing\n",
        "\n",
        "\n",
        "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
        "\n",
        "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
        "prices and the demand for clean air', J. Environ. Economics & Management,\n",
        "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
        "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
        "pages 244-261 of the latter.\n",
        "\n",
        "The Boston house-price data has been used in many machine learning papers that address regression\n",
        "problems.   \n",
        "     \n",
        "**References**\n",
        "\n",
        "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
        "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
        "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
        "\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>CRIM</th>\n",
        "      <th>ZN</th>\n",
        "      <th>INDUS</th>\n",
        "      <th>CHAS</th>\n",
        "      <th>NOX</th>\n",
        "      <th>RM</th>\n",
        "      <th>AGE</th>\n",
        "      <th>DIS</th>\n",
        "      <th>RAD</th>\n",
        "      <th>TAX</th>\n",
        "      <th>PTRATIO</th>\n",
        "      <th>B</th>\n",
        "      <th>LSTAT</th>\n",
        "      <th>MEDV</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 0.00632</td>\n",
        "      <td> 18</td>\n",
        "      <td> 2.31</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.538</td>\n",
        "      <td> 6.575</td>\n",
        "      <td> 65.2</td>\n",
        "      <td> 4.0900</td>\n",
        "      <td> 1</td>\n",
        "      <td> 296</td>\n",
        "      <td> 15.3</td>\n",
        "      <td> 396.90</td>\n",
        "      <td> 4.98</td>\n",
        "      <td> 24.0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 0.02731</td>\n",
        "      <td>  0</td>\n",
        "      <td> 7.07</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.469</td>\n",
        "      <td> 6.421</td>\n",
        "      <td> 78.9</td>\n",
        "      <td> 4.9671</td>\n",
        "      <td> 2</td>\n",
        "      <td> 242</td>\n",
        "      <td> 17.8</td>\n",
        "      <td> 396.90</td>\n",
        "      <td> 9.14</td>\n",
        "      <td> 21.6</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 0.02729</td>\n",
        "      <td>  0</td>\n",
        "      <td> 7.07</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.469</td>\n",
        "      <td> 7.185</td>\n",
        "      <td> 61.1</td>\n",
        "      <td> 4.9671</td>\n",
        "      <td> 2</td>\n",
        "      <td> 242</td>\n",
        "      <td> 17.8</td>\n",
        "      <td> 392.83</td>\n",
        "      <td> 4.03</td>\n",
        "      <td> 34.7</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 0.03237</td>\n",
        "      <td>  0</td>\n",
        "      <td> 2.18</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.458</td>\n",
        "      <td> 6.998</td>\n",
        "      <td> 45.8</td>\n",
        "      <td> 6.0622</td>\n",
        "      <td> 3</td>\n",
        "      <td> 222</td>\n",
        "      <td> 18.7</td>\n",
        "      <td> 394.63</td>\n",
        "      <td> 2.94</td>\n",
        "      <td> 33.4</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 0.06905</td>\n",
        "      <td>  0</td>\n",
        "      <td> 2.18</td>\n",
        "      <td> 0</td>\n",
        "      <td> 0.458</td>\n",
        "      <td> 7.147</td>\n",
        "      <td> 54.2</td>\n",
        "      <td> 6.0622</td>\n",
        "      <td> 3</td>\n",
        "      <td> 222</td>\n",
        "      <td> 18.7</td>\n",
        "      <td> 396.90</td>\n",
        "      <td> 5.33</td>\n",
        "      <td> 36.2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "      CRIM  ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO       B  LSTAT  MEDV\n",
        "0  0.00632  18   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3  396.90   4.98  24.0\n",
        "1  0.02731   0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8  396.90   9.14  21.6\n",
        "2  0.02729   0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8  392.83   4.03  34.7\n",
        "3  0.03237   0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7  394.63   2.94  33.4\n",
        "4  0.06905   0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7  396.90   5.33  36.2"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_columns = list(bostondf.columns) ## designate independent variables\n",
      "y_column = 'MEDV' ## designate dependent (target) variable\n",
      "x_columns.remove(y_column) ## remove the y column from the list of x columns programmatically\n",
      "\n",
      "significant_columns = [] ## create an empty list for storing significant columns\n",
      "junk = []\n",
      "pvals = [] ## create an empty list for storing p-values\n",
      "\n",
      "for feature in x_columns:  ## for each feature\n",
      "    pval = f_select.f_regression(bostondf[[feature]], bostondf[y_column])  ## run a regression wrt the target variable and store into pval object\n",
      "    #print pval\n",
      "    #print pval[1][0]\n",
      "    if pval[1][0] < 0.05: ## if the pvalue, located at [1][0], is significant (< 0.05)...\n",
      "        significant_columns.append(feature) ## add the feature name to the \"significant_columns\" list\n",
      "        pvals.append(pval[1][0]) ## add the p-value to the \"pvals\" lists\n",
      "    else:\n",
      "        junk.append(feature)\n",
      "        \n",
      "print \"significant_columns:\"\n",
      "print significant_columns\n",
      "print\n",
      "print \"junk:\"\n",
      "print junk\n",
      "print\n",
      "print \"pvals:\"\n",
      "print pvals"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "significant_columns:\n",
        "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "junk:\n",
        "[]\n",
        "\n",
        "pvals:\n",
        "[2.0835501108141935e-19, 5.7135841530816856e-17, 4.9002599817531711e-31, 7.3906231705208155e-05, 7.0650415862543328e-24, 2.4872288710081537e-74, 1.5699822091882311e-18, 1.2066117273372841e-08, 5.4659325696485665e-19, 5.6377336276914977e-29, 1.6095094784730471e-34, 1.3181127340756421e-14, 5.0811033943885684e-88]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## define training and testing variables, generates two objects for each side of the equation\n",
      "## train variables define the model (significant_columns)\n",
      "## test variables tests effectiveness of the model using a randomized subset of the data\n",
      "x_train, x_test, y_train, y_test = cv.train_test_split(bostondf[significant_columns],\n",
      "                                                           bostondf[y_column],\n",
      "                                                           test_size=0.333, ## 33% of training data size\n",
      "                                                           random_state=1234) ## random seed variable\n",
      "\n",
      "## create a linear regression model, fitting the significant columns to the target variable\n",
      "model = lm.LinearRegression().fit(x_train, y_train)\n",
      "\n",
      "## create a dataframe to display the coefficient scores and p-values of each feature\n",
      "print pd.DataFrame({\n",
      "    'column': significant_columns,\n",
      "    'coef': model.coef_,\n",
      "    'p-value': pvals,\n",
      "}).set_index('column')\n",
      "\n",
      "## score the model's effectiveness, comparing training dataset to testing dataset\n",
      "## did it perform better than random?\n",
      "print \"Train Data Score:\"\n",
      "print model.score(x_train, y_train)\n",
      "print metrics.r2_score(y_train, model.predict(x_train))\n",
      "print \"Test Data Score\"\n",
      "print model.score(x_test, y_test)\n",
      "print metrics.r2_score(y_test, model.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              coef       p-value\n",
        "column                          \n",
        "CRIM     -0.101202  2.083550e-19\n",
        "ZN        0.062922  5.713584e-17\n",
        "INDUS    -0.024451  4.900260e-31\n",
        "CHAS      2.743707  7.390623e-05\n",
        "NOX     -22.724112  7.065042e-24\n",
        "RM        2.415003  2.487229e-74\n",
        "AGE       0.004942  1.569982e-18\n",
        "DIS      -1.904339  1.206612e-08\n",
        "RAD       0.389172  5.465933e-19\n",
        "TAX      -0.014100  5.637734e-29\n",
        "PTRATIO  -1.118604  1.609509e-34\n",
        "B         0.007027  1.318113e-14\n",
        "LSTAT    -0.587270  5.081103e-88\n",
        "Train Data Score:\n",
        "0.72839057184\n",
        "0.72839057184\n",
        "Test Data Score\n",
        "0.736360506263\n",
        "0.736360506263\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Data Pipeline\n",
      "1. Load\n",
      "2. Clean\n",
      "3. Reshape/Subset\n",
      "4. Split (group values into target and inputs variables)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print pd.DataFrame({\n",
      "    'column': significant_columns,\n",
      "    'coef': model.coef_,\n",
      "    'p-value': pvals,\n",
      "}).set_index('column')\n",
      "\n",
      "\"\"\"\n",
      "coefficients of each feature tells us how it is related to target variable \n",
      "    - each feature == mx + b wrt y\n",
      "\n",
      "for example: \n",
      "    - as nitric oxides go down, median value goes up\n",
      "    - the median value of properties closer to the Charles River is higher than those further away\n",
      "\"\"\"\n",
      "\"\"\"\n",
      "p-value tells us the \"P\"robability that two datasets are NOT random\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "              coef       p-value\n",
        "column                          \n",
        "CRIM     -0.101202  2.083550e-19\n",
        "ZN        0.062922  5.713584e-17\n",
        "INDUS    -0.024451  4.900260e-31\n",
        "CHAS      2.743707  7.390623e-05\n",
        "NOX     -22.724112  7.065042e-24\n",
        "RM        2.415003  2.487229e-74\n",
        "AGE       0.004942  1.569982e-18\n",
        "DIS      -1.904339  1.206612e-08\n",
        "RAD       0.389172  5.465933e-19\n",
        "TAX      -0.014100  5.637734e-29\n",
        "PTRATIO  -1.118604  1.609509e-34\n",
        "B         0.007027  1.318113e-14\n",
        "LSTAT    -0.587270  5.081103e-88\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "'\\np-value tells us the \"P\"robability that two datasets are NOT random\\n'"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print model.predict(x_train) ## where the model predicts y to be given x value\n",
      "#print np.mean(y_train) ## mean of y value\n",
      "sum_squares_regr = np.sum((model.predict(x_train) - np.mean(y_train))**2)  \n",
      "## takes the difference between where the model predicts the y-value to be at each x-value and the mean y-value\n",
      "## squares the difference\n",
      "## sums them up\n",
      "\n",
      "#print y_train ## all y-values\n",
      "#print np.mean(y_train) ## mean of y-values\n",
      "total_sum_squares = np.sum((y_train - np.mean(y_train))**2) \n",
      "## takes the difference between each y-value and the mean of all y-values\n",
      "## squares the difference\n",
      "## sums them up\n",
      "\n",
      "r_squared = sum_squares_regr / total_sum_squares  \n",
      "## r-squared is the ratio between these two: R^2 = SSR/TSS\n",
      "## see explanation in lecture notes\n",
      "\n",
      "print sum_squares_regr\n",
      "print total_sum_squares\n",
      "print r_squared"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20130.3658381\n",
        "27636.7743027\n",
        "0.72839057184\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Machine Learning Workflow\n",
      "1. Data Pipline \n",
      "    - preprocessing data (get/clean/transform)\n",
      "    - bad diagnostics? see if a variable needs to be changed\n",
      "2. Feature Selection\n",
      "    - define the target variable (y)\n",
      "    - define variables of interest that determine y\n",
      "3. Cross Validation\n",
      "    - define test data vs. training data\n",
      "    - repeat multiple times\n",
      "4. Apply a Regression\n",
      "    - linear, for example\n",
      "    - bad diagnostics? try another\n",
      "5. Learning Diagnostics\n",
      "    - evaluate metrics\n",
      "    - how useful, if at all, is this model?\n",
      "6. Repeat    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}